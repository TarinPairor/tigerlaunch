<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Conversation Companion (Realtime)</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }

        .container {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            max-width: 600px;
            width: 100%;
            padding: 30px;
        }

        h1 {
            text-align: center;
            color: #333;
            margin-bottom: 30px;
            font-size: 28px;
        }

        .status {
            text-align: center;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
            font-weight: 500;
        }

        .status.listening {
            background: #e3f2fd;
            color: #1976d2;
        }

        .status.processing {
            background: #fff3e0;
            color: #f57c00;
        }

        .status.speaking {
            background: #f3e5f5;
            color: #7b1fa2;
        }

        .status.idle {
            background: #f5f5f5;
            color: #666;
        }

        .status.error {
            background: #ffebee;
            color: #c62828;
        }

        .status.complete {
            background: #e8f5e9;
            color: #2e7d32;
        }

        .button-group {
            display: flex;
            gap: 15px;
            margin-bottom: 30px;
        }

        button {
            flex: 1;
            padding: 15px 25px;
            border: none;
            border-radius: 10px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        button:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .start-btn {
            background: #4caf50;
            color: white;
        }

        .stop-btn {
            background: #f44336;
            color: white;
        }

        .message-box {
            background: #f9f9f9;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
            min-height: 100px;
            max-height: 300px;
            overflow-y: auto;
        }

        .message {
            margin-bottom: 15px;
            padding: 12px;
            border-radius: 8px;
        }

        .message.user {
            background: #e3f2fd;
            margin-left: 20%;
        }

        .message.assistant {
            background: #f1f8e9;
            margin-right: 20%;
        }

        .message-label {
            font-weight: 600;
            margin-bottom: 5px;
            font-size: 12px;
            text-transform: uppercase;
            opacity: 0.7;
        }

        .message-text {
            color: #333;
            line-height: 1.6;
        }

        .message-metrics {
            font-size: 11px;
            color: #666;
            margin-top: 8px;
            padding-top: 8px;
            border-top: 1px solid rgba(0,0,0,0.1);
            font-style: italic;
        }

        .config {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
        }

        .config input {
            width: 100%;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            margin-top: 5px;
            font-size: 14px;
        }

        .config label {
            display: block;
            margin-bottom: 10px;
            font-weight: 500;
            color: #555;
        }

        .pulse {
            animation: pulse 1.5s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% {
                opacity: 1;
            }
            50% {
                opacity: 0.5;
            }
        }

        .pipeline-nav {
            text-align: center;
            margin-top: 40px;
            padding-top: 30px;
            border-top: 2px solid #e0e0e0;
        }

        .pipeline-step {
            font-size: 14px;
            color: #666;
            margin-bottom: 20px;
            font-weight: 500;
        }

        .next-arrow {
            display: inline-flex;
            flex-direction: column;
            align-items: center;
            text-decoration: none;
            color: #667eea;
            transition: all 0.3s;
            padding: 15px 30px;
            border-radius: 12px;
            background: #f5f5ff;
            border: 2px solid #667eea;
        }

        .next-arrow:hover {
            background: #667eea;
            color: white;
            transform: translateY(-3px);
            box-shadow: 0 5px 20px rgba(102, 126, 234, 0.4);
        }

        .next-arrow-icon {
            font-size: 36px;
            animation: arrowPulse 2s ease-in-out infinite;
            display: block;
            margin-bottom: 8px;
        }

        @keyframes arrowPulse {
            0%, 100% {
                transform: translateX(0);
            }
            50% {
                transform: translateX(8px);
            }
        }

        .next-arrow-text {
            font-size: 16px;
            font-weight: 600;
            display: block;
        }

        .nav-links {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            gap: 10px;
            z-index: 1000;
        }

        .dashboard-link {
            padding: 12px 24px;
            background: #2196f3;
            color: white;
            text-decoration: none;
            border-radius: 10px;
            font-weight: 600;
            font-size: 14px;
            box-shadow: 0 3px 12px rgba(0,0,0,0.2);
            transition: all 0.3s;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .dashboard-link:hover {
            background: #1976d2;
            transform: translateY(-2px);
            box-shadow: 0 5px 18px rgba(0,0,0,0.3);
        }

        .audio-player {
            margin-top: 20px;
            text-align: center;
            background: #f9f9f9;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .audio-player audio {
            width: 100%;
            max-width: 500px;
        }

        .voice-analysis-results {
            display: none;
            background: #f9f9f9;
            border-radius: 10px;
            padding: 25px;
            margin-top: 30px;
        }

        .voice-analysis-results.active {
            display: block;
        }

        .voice-analysis-results h2 {
            color: #333;
            margin-bottom: 20px;
            font-size: 22px;
        }

        .result-item {
            background: white;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 15px;
            border-left: 4px solid #667eea;
        }

        .result-label {
            font-weight: 600;
            color: #666;
            margin-bottom: 5px;
            font-size: 14px;
            text-transform: uppercase;
        }

        .result-value {
            font-size: 24px;
            font-weight: bold;
            color: #333;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .result-icon {
            font-size: 32px;
        }

        .result-icon.healthy {
            color: #4caf50;
        }

        .result-icon.mci {
            color: #ff9800;
        }

        .description-modal {
            background: #f0f7ff;
            border-left: 4px solid #2196f3;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            font-size: 14px;
            color: #333;
            line-height: 1.6;
        }

        .description-modal h4 {
            margin: 0 0 8px 0;
            color: #1976d2;
            font-size: 16px;
        }

        .result-probabilities {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #e0e0e0;
        }

        .probability-bar-container {
            margin-bottom: 20px;
        }

        .probability-bar-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 8px;
            font-size: 14px;
            font-weight: 500;
            color: #666;
        }

        .probability-bar-wrapper {
            width: 100%;
            height: 30px;
            background: #e0e0e0;
            border-radius: 15px;
            overflow: hidden;
            position: relative;
        }

        .probability-bar {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            border-radius: 15px;
            transition: width 1s ease-out;
            display: flex;
            align-items: center;
            justify-content: flex-end;
            padding-right: 10px;
            color: white;
            font-weight: 600;
            font-size: 12px;
            animation: barLoad 1s ease-out;
        }

        .probability-bar.healthy {
            background: linear-gradient(90deg, #4caf50 0%, #66bb6a 100%);
        }

        .probability-bar.mci {
            background: linear-gradient(90deg, #ff9800 0%, #ffb74d 100%);
        }

        @keyframes barLoad {
            0% {
                width: 0% !important;
            }
        }
    </style>
</head>
<body>
    <div class="nav-links">
        <a href="/" class="dashboard-link">üè† Home</a>
        <a href="/dashboard" class="dashboard-link">üìä View Dashboard</a>
    </div>
    
    <div class="container">
        <div style="text-align: center; margin-bottom: 20px;">
            <div style="font-size: 12px; color: #666; margin-bottom: 10px; text-transform: uppercase; letter-spacing: 1px;">Step 1 of 3</div>
            <h1 style="margin: 0;">üéôÔ∏è Start Your Conversation</h1>
            <p style="color: #666; margin-top: 10px; font-size: 16px;">Have a natural conversation with our AI companion. Speak freely and take your time.</p>
        </div>
        
        <div class="config">
            <label>
                AI Language:
                <div style="margin-top: 5px;">
                    <button id="languageToggle" onclick="toggleLanguage()" style="padding: 8px 16px; font-size: 14px; background: #667eea; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: 600;">
                        <span id="languageLabel">English</span> <span id="languageFlag">üá∫üá∏</span>
                    </button>
                </div>
            </label>
            <label style="margin-top: 15px;">
                Silence Wait Time (ms):
                <input type="number" id="silenceDuration" value="0" min="0" max="10000" step="500">
                <small style="display: block; margin-top: 5px; color: #666; font-weight: normal;">
                    How long to wait after silence before responding. 0ms = no delay (original behavior). Higher values = more patient for slower speakers.
                </small>
            </label>
            <div style="margin-top: 10px; display: flex; gap: 10px; flex-wrap: wrap;">
                <button onclick="setSilenceDuration(0)" style="padding: 5px 10px; font-size: 12px; background: #4caf50; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;">No Delay (0ms)</button>
                <button onclick="setSilenceDuration(1500)" style="padding: 5px 10px; font-size: 12px; background: #e0e0e0; border: none; border-radius: 5px; cursor: pointer;">Fast (1.5s)</button>
                <button onclick="setSilenceDuration(3000)" style="padding: 5px 10px; font-size: 12px; background: #e0e0e0; border: none; border-radius: 5px; cursor: pointer;">Normal (3s)</button>
                <button onclick="setSilenceDuration(5000)" style="padding: 5px 10px; font-size: 12px; background: #e0e0e0; border: none; border-radius: 5px; cursor: pointer;">Slow (5s)</button>
                <button onclick="setSilenceDuration(7000)" style="padding: 5px 10px; font-size: 12px; background: #e0e0e0; border: none; border-radius: 5px; cursor: pointer;">Very Slow (7s)</button>
            </div>
        </div>

        <div id="status" class="status idle">Ready to start your conversation</div>

        <div class="button-group">
            <button id="startBtn" class="start-btn" onclick="startConversation()">Start Conversation</button>
            <button id="stopBtn" class="stop-btn" onclick="stopConversation()" disabled>End Conversation</button>
        </div>

        <div class="message-box" id="messages">
            <div style="text-align: center; color: #999; padding: 20px;">
                Your conversation will appear here once you start...
            </div>
        </div>

        <div class="audio-player" id="audioPlayer" style="display: none;">
            <p style="margin-bottom: 10px; color: #666;">Recorded Audio:</p>
            <audio id="audioPlayback" controls></audio>
        </div>

        <div class="pipeline-nav">
            <div class="pipeline-step">When you're ready, continue to the next step</div>
            <a href="/tics_assessment" class="next-arrow">
                <span class="next-arrow-icon">‚Üí</span>
                <span class="next-arrow-text">Cognitive Assessment</span>
            </a>
        </div>

        <div class="voice-analysis-results" id="voiceAnalysisResults">
            <h2>Voice Analysis Results</h2>
            <div class="result-item">
                <div class="result-label">Voice Analysis Result</div>
                <div class="result-value" id="predictedClass">-</div>
            </div>
            <div class="description-modal" id="descriptionModal" style="display: none;">
                <h4 id="descriptionTitle"></h4>
                <p id="descriptionText"></p>
            </div>
            <div class="result-item" id="probabilitiesSection" style="display: none;">
                <div class="result-label">Detailed Probabilities</div>
                <div class="result-probabilities" id="probabilities"></div>
            </div>
        </div>
    </div>

    <script>
        let peerConnection = null;
        let audioElement = null;
        let dataChannel = null;
        let userAudioTrack = null;
        let userStream = null;
        let serverAudioStream = null;
        let isSessionActive = false;
        let messageText = '';
        let userSpeakingStartTime = null;
        let aiResponseStartTime = null;
        let currentUserTranscript = '';
        let audioBlob = null;
        let mediaRecorder = null;
        let isRecording = false;
        let audioSegments = []; // Array to store audio segments from each listening period
        let currentSegmentChunks = []; // Chunks for current segment
        const API_ENDPOINT = 'http://localhost:5001/analyze';
        let currentLanguage = 'en'; // 'en' for English, 'zh' for Chinese
        
        // Conversation metrics tracking
        let sessionStartTime = null;
        let sessionEndTime = null;
        let userMessages = [];
        let totalWordsSpoken = 0;
        let totalSpeakingDuration = 0;

        // Get API key from server
        async function getApiKey() {
            try {
                const response = await fetch('/api-key');
                if (response.ok) {
                    const data = await response.json();
                    if (data.apiKey) {
                        return data.apiKey;
                    }
                }
            } catch (e) {
                console.log('Could not load API key from server');
            }

            return null;
        }

        // Get ephemeral key for Realtime API
        async function getEphemeralKey(apiKey) {
            try {
                // Try to get from server endpoint
                const response = await fetch('/token');
                if (response.ok) {
                    const data = await response.json();
                    return data.client_secret?.value || data.clientSecret?.value;
                }
            } catch (e) {
                console.log('Server token endpoint not available, using API key directly');
            }
            
            // Fallback: use API key directly (may not work for Realtime API)
            return apiKey;
        }

        // Start conversation with Realtime API
        async function startConversation() {
            const apiKey = await getApiKey();
            
            if (!apiKey) {
                alert('Please create a .env file with OPENAI_API_KEY=your-key');
                return;
            }

            try {
                updateStatus('processing', 'üîÑ Connecting...');
                
                // Get ephemeral key
                const ephemeralKey = await getEphemeralKey(apiKey);
                if (!ephemeralKey) {
                    throw new Error('Could not get ephemeral key');
                }

                // Create peer connection
                peerConnection = new RTCPeerConnection();

                // Create audio element for server audio
                audioElement = document.createElement('audio');
                audioElement.autoplay = true;
                audioElement.muted = false;
                document.body.appendChild(audioElement);

                // Handle incoming server audio
                peerConnection.ontrack = (event) => {
                    if (event.track.kind === 'audio' && event.streams && event.streams[0]) {
                        serverAudioStream = event.streams[0];
                        audioElement.srcObject = event.streams[0];
                        audioElement.play().catch(e => {
                            console.error('Audio play failed:', e);
                        });
                        updateStatus('speaking', 'üîä AI is speaking...');
                    }
                };

                // Create data channel for events
                dataChannel = peerConnection.createDataChannel('oai-events', { ordered: true });
                
                dataChannel.onopen = () => {
                    console.log('Data channel opened');
                    isSessionActive = true;
                    updateStatus('listening', 'üé§ Listening...');
                    
                    // Get silence duration from input (default 0ms for no delay)
                    const silenceDurationInput = document.getElementById('silenceDuration').value;
                    const silenceDuration = silenceDurationInput !== '' ? parseInt(silenceDurationInput) : 0;
                    
                    // Get instructions based on language
                    let instructions;
                    if (currentLanguage === 'zh') {
                        instructions = silenceDuration === 0 
                            ? '‰Ω†ÊòØ‰∏Ä‰∏™ÂèãÂ•Ω„ÄÅ‰πê‰∫éÂä©‰∫∫ÁöÑ AI ‰ºô‰º¥„ÄÇËØ∑‰øùÊåÅÂõûÁ≠îÁÆÄÊ¥Å„ÄÅËá™ÁÑ∂ÔºåÈÄÇÂêàËØ≠Èü≥‰∫§ÊµÅÔºåÂπ∂Âú®ÂêàÈÄÇÁöÑÊÉÖÂÜµ‰∏ã‰∏ªÂä®Áª¥ÊåÅÂØπËØùÁöÑËøõË°å„ÄÇÊØèÊ¨°ÂõûÂ§çÊéßÂà∂Âú® 2‚Äì3 Âè•ËØù‰ª•ÂÜÖ„ÄÇ'
                            : '‰Ω†ÊòØ‰∏Ä‰∏™ÂèãÂ•Ω„ÄÅ‰πê‰∫éÂä©‰∫∫ÁöÑ AI ‰ºô‰º¥ÔºåÊ≠£Âú®‰∏é‰∏Ä‰ΩçÂèØËÉΩËØ¥ËØùËæÉÊÖ¢ÁöÑËÄÅÂπ¥‰∫∫‰∫§ÊµÅ„ÄÇËØ∑‰øùÊåÅËÄêÂøÉÔºåÁ≠âÂæÖ‰ªñ‰ª¨ÂÆåÂÖ®ËØ¥ÂÆåÂêéÂÜçÂõûÂ∫îÔºåÂπ∂Âú®ÂêàÈÄÇÁöÑÊÉÖÂÜµ‰∏ã‰∏ªÂä®Áª¥ÊåÅÂØπËØùÁöÑËøõË°å„ÄÇ‰øùÊåÅÂõûÁ≠îÁÆÄÊ¥Å„ÄÅËá™ÁÑ∂ÔºåÊØèÊ¨°ÂõûÂ§çÊéßÂà∂Âú® 2‚Äì3 Âè•ËØù‰ª•ÂÜÖ„ÄÇ';
                    } else {
                        instructions = silenceDuration === 0 
                            ? 'You are a friendly, helpful AI companion. Keep responses concise and conversational, suitable for voice interaction, and try to keep the conversation going. Limit responses to 2‚Äì3 sentences.'
                            : 'You are a friendly, helpful AI companion speaking with an elderly person who may speak slowly. Be patient, wait for them to finish speaking completely, and try to keep the conversation going. Keep responses concise and conversational. Limit responses to 2‚Äì3 sentences.';
                    }

                    
                    // Build session configuration
                    const sessionConfig = {
                        modalities: ['text', 'audio'],
                        instructions: instructions,
                        input_audio_transcription: {
                            model: 'whisper-1'
                        },
                        turn_detection: {
                            type: 'client_vad',
                            threshold: 0.7,
                            prefix_padding_ms: 300,
                            silence_duration_ms: 2000
                        }
                    };
                    
                    // Only add turn detection if silence duration is > 0 (for delay)
                    if (silenceDuration > 0) {
                        sessionConfig.turn_detection = {
                            type: 'server_vad',
                            threshold: 0.7,
                            prefix_padding_ms: 300,
                            silence_duration_ms: silenceDuration
                        };
                    }
                    
                    // Send session configuration
                    const event = {
                        type: 'session.update',
                        session: sessionConfig
                    };
                    dataChannel.send(JSON.stringify(event));
                    
                    // Enable user audio
                    if (userAudioTrack) {
                        userAudioTrack.enabled = true;
                    }
                };

                dataChannel.onmessage = (event) => {
                    const message = JSON.parse(event.data);
                    handleMessage(message);
                };

                // Get user microphone
                userStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                userAudioTrack = userStream.getAudioTracks()[0];
                peerConnection.addTrack(userAudioTrack, userStream);
                
                // Start with mic enabled for continuous listening
                userAudioTrack.enabled = true;

                // MediaRecorder will be started/stopped based on listening state
                // Don't start recording yet - wait for listening state

                // Create offer
                const offer = await peerConnection.createOffer();
                await peerConnection.setLocalDescription(offer);

                // Send offer to OpenAI Realtime API
                const sdpRes = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17', {
                    method: 'POST',
                    body: offer.sdp,
                    headers: {
                        'Authorization': `Bearer ${ephemeralKey}`,
                        'Content-Type': 'application/sdp'
                    }
                });

                if (!sdpRes.ok) {
                    const errorText = await sdpRes.text();
                    throw new Error(`SDP exchange failed: ${sdpRes.status} - ${errorText}`);
                }

                // Set remote answer
                const answerSdp = await sdpRes.text();
                await peerConnection.setRemoteDescription({ type: 'answer', sdp: answerSdp });

                document.getElementById('startBtn').disabled = true;
                document.getElementById('stopBtn').disabled = false;
                document.getElementById('languageToggle').disabled = true;
                clearMessages();
                addMessage('system', 'Connected! You can now start speaking naturally...');
                
                // Initialize session tracking
                sessionStartTime = Date.now();
                userMessages = [];
                totalWordsSpoken = 0;
                totalSpeakingDuration = 0;
                audioSegments = [];
                currentSegmentChunks = [];
                isRecording = false;

            } catch (error) {
                console.error('Error starting session:', error);
                updateStatus('error', `Error: ${error.message}`);
                stopConversation();
            }
        }

        // Start recording user audio segment (during listening state)
        function startRecordingSegment() {
            if (!userStream || isRecording) {
                return;
            }

            try {
                currentSegmentChunks = [];
                mediaRecorder = new MediaRecorder(userStream);
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        currentSegmentChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = () => {
                    // Save this segment
                    if (currentSegmentChunks.length > 0) {
                        const segmentBlob = new Blob(currentSegmentChunks, { type: 'audio/webm' });
                        audioSegments.push(segmentBlob);
                        console.log(`‚úÖ Saved audio segment ${audioSegments.length}, size: ${segmentBlob.size} bytes`);
                    }
                    currentSegmentChunks = [];
                };

                mediaRecorder.start();
                isRecording = true;
                console.log('üé§ Started recording user audio segment');
            } catch (error) {
                console.error('Error starting audio recording:', error);
            }
        }

        // Stop recording user audio segment (when user stops speaking)
        function stopRecordingSegment() {
            if (mediaRecorder && isRecording && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                isRecording = false;
                console.log('üõë Stopped recording user audio segment');
            }
        }

        // Handle messages from Realtime API
        function handleMessage(message) {
            // Debug: log all message types to see what we're getting
            if (message.type && message.type.includes('input_audio') || message.type.includes('transcription')) {
                console.log('Audio transcription event:', message.type, message);
            }
            
            if (message.type === 'response.audio_transcript.delta' && message.delta) {
                messageText += message.delta;
                updateMessageText(messageText);
            } else if (message.type === 'response.audio_transcript.done') {
                // Final transcript received - remove streaming message and add final one
                if (messageText) {
                    // Remove any existing streaming assistant message
                    removeStreamingAssistantMessage();
                    // Add final message
                    addMessage('assistant', messageText);
                    messageText = '';
                }
                updateStatus('listening', 'üé§ Listening...');
                // Start recording when we enter listening state (user will speak)
                startRecordingSegment();
            } else if (message.type === 'conversation.item.input_audio_transcription.started') {
                // User started speaking - ensure we're recording
                if (!isRecording) {
                    startRecordingSegment();
                }
            } else if (message.type === 'conversation.item.input_audio_transcription.completed') {
                // User finished speaking - stop recording this segment
                stopRecordingSegment();
                
                // User speech transcribed
                const transcript = message.transcript;
                if (transcript) {
                    // Calculate metrics
                    const wordCount = transcript.trim().split(/\s+/).filter(word => word.length > 0).length;
                    
                    // Calculate duration from when user started speaking
                    let speakingDuration = 'N/A';
                    const transcriptionCompleteTime = Date.now();
                    
                    // Try multiple ways to get the start time
                    let actualStartTime = userSpeakingStartTime;
                    
                    // Check if message has item with created_at or started_at
                    if (!actualStartTime && message.item) {
                        if (message.item.created_at) {
                            actualStartTime = new Date(message.item.created_at).getTime();
                        } else if (message.item.started_at) {
                            actualStartTime = new Date(message.item.started_at).getTime();
                        }
                    }
                    
                    // Check if message itself has timestamp
                    if (!actualStartTime && message.created_at) {
                        actualStartTime = new Date(message.created_at).getTime();
                    }
                    
                    console.log('Transcription completed:', {
                        userSpeakingStartTime,
                        actualStartTime,
                        aiResponseStartTime,
                        transcriptionCompleteTime,
                        messageItem: message.item
                    });
                    
                    // Calculate words per second
                    let wordsPerSecond = null;
                    let durationSeconds = 0;
                    
                    if (actualStartTime) {
                        // Use AI response time if AI cut in while user was speaking, 
                        // otherwise use transcription complete time (when user finished speaking)
                        const endTime = (aiResponseStartTime && aiResponseStartTime < transcriptionCompleteTime && aiResponseStartTime > actualStartTime) 
                            ? aiResponseStartTime 
                            : transcriptionCompleteTime;
                        const durationMs = endTime - actualStartTime;
                        console.log('Calculated duration:', durationMs, 'ms from', actualStartTime, 'to', endTime);
                        if (durationMs > 0) {
                            durationSeconds = durationMs / 1000;
                            speakingDuration = durationSeconds.toFixed(2) + 's';
                            
                            // Calculate words per second
                            if (wordCount > 0 && durationSeconds > 0) {
                                wordsPerSecond = parseFloat((wordCount / durationSeconds).toFixed(2));
                            }
                        }
                    }
                    
                    // Track metrics
                    totalWordsSpoken += wordCount;
                    if (durationSeconds > 0) {
                        totalSpeakingDuration += durationSeconds;
                    }
                    
                    // Store user message data
                    userMessages.push({
                        wordCount: wordCount,
                        duration: durationSeconds,
                        speechRate: wordsPerSecond
                    });
                    
                    addMessage('user', transcript, wordCount, speakingDuration, wordsPerSecond);
                    updateStatus('processing', 'ü§î AI is thinking...');
                    
                    // Reset tracking AFTER displaying message
                    userSpeakingStartTime = null;
                    aiResponseStartTime = null;
                    currentUserTranscript = '';
                }
            } else if (message.type === 'conversation.item.input_audio_transcription.delta' || 
                       message.type === 'input_audio_transcription.delta' ||
                       (message.type === 'conversation.item.input_audio_transcription' && message.delta)) {
                // User is speaking - track start time on first delta
                if (!userSpeakingStartTime) {
                    userSpeakingStartTime = Date.now();
                    console.log('User started speaking at:', userSpeakingStartTime);
                    aiResponseStartTime = null; // Reset AI response time
                }
                if (message.delta) {
                    currentUserTranscript += message.delta;
                }
            } else if (message.type === 'conversation.item.created' && message.item && message.item.type === 'input_audio_transcription') {
                // Track when transcription item is created (user started speaking)
                if (!userSpeakingStartTime) {
                    if (message.item.created_at) {
                        userSpeakingStartTime = new Date(message.item.created_at).getTime();
                    } else {
                        userSpeakingStartTime = Date.now();
                    }
                    console.log('User started speaking (item created) at:', userSpeakingStartTime);
                    aiResponseStartTime = null;
                }
            } else if (message.type && message.type.includes('input_audio') && !message.type.includes('completed')) {
                // Catch other input audio events to track start time
                if (!userSpeakingStartTime) {
                    userSpeakingStartTime = Date.now();
                    console.log('User started speaking (alternative event) at:', userSpeakingStartTime, message.type);
                }
            } else if (message.type === 'response.audio.delta') {
                // Audio is being streamed - AI is responding
                // This is when AI cuts in (if user was still speaking)
                if (userSpeakingStartTime && !aiResponseStartTime) {
                    aiResponseStartTime = Date.now();
                    console.log('AI started responding at:', aiResponseStartTime, 'User started at:', userSpeakingStartTime);
                }
                updateStatus('speaking', 'üîä AI is speaking...');
                // Stop recording if AI starts speaking (in case user was still speaking)
                stopRecordingSegment();
            } else if (message.type === 'response.audio.done') {
                // Audio finished
                updateStatus('listening', 'üé§ Listening...');
                // Start recording when we enter listening state
                startRecordingSegment();
            }
        }

        // Remove streaming assistant message (if exists)
        function removeStreamingAssistantMessage() {
            const messagesDiv = document.getElementById('messages');
            const messages = Array.from(messagesDiv.children);
            
            // Find and remove the last assistant message if it's marked as streaming
            for (let i = messages.length - 1; i >= 0; i--) {
                const msg = messages[i];
                if (msg.classList.contains('assistant') && msg.hasAttribute('data-streaming')) {
                    msg.remove();
                    break;
                }
            }
        }
        
        // Update streaming message text
        function updateMessageText(text) {
            const messagesDiv = document.getElementById('messages');
            const messages = Array.from(messagesDiv.children);
            
            // Find the last streaming assistant message
            let streamingMessage = null;
            for (let i = messages.length - 1; i >= 0; i--) {
                const msg = messages[i];
                if (msg.classList.contains('assistant') && msg.hasAttribute('data-streaming')) {
                    streamingMessage = msg;
                    break;
                }
            }
            
            if (streamingMessage) {
                // Update existing streaming message
                const textDiv = streamingMessage.querySelector('.message-text');
                if (textDiv) {
                    textDiv.textContent = text;
                }
            } else {
                // Create new streaming message with data attribute
                const messageDiv = document.createElement('div');
                messageDiv.className = 'message assistant';
                messageDiv.setAttribute('data-streaming', 'true');
                
                const labelDiv = document.createElement('div');
                labelDiv.className = 'message-label';
                labelDiv.textContent = 'ü§ñ Assistant';
                
                const textDiv = document.createElement('div');
                textDiv.className = 'message-text';
                textDiv.textContent = text;
                
                messageDiv.appendChild(labelDiv);
                messageDiv.appendChild(textDiv);
                messagesDiv.appendChild(messageDiv);
            }
            
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }

        // Stop conversation
        async function stopConversation() {
            sessionEndTime = Date.now();
            
            if (dataChannel) {
                dataChannel.close();
                dataChannel = null;
            }

            if (peerConnection) {
                peerConnection.getSenders().forEach(sender => {
                    sender.track?.stop();
                });
                peerConnection.close();
                peerConnection = null;
            }

            if (audioElement) {
                audioElement.pause();
                audioElement.srcObject = null;
                audioElement.remove();
                audioElement = null;
            }

            // Stop any ongoing recording segment
            stopRecordingSegment();

            // Wait a bit for the last segment to finish saving
            await new Promise(resolve => setTimeout(resolve, 500));

            // Combine all audio segments into one continuous audio stream
            if (audioSegments.length > 0) {
                console.log(`üîó Combining ${audioSegments.length} audio segments...`);
                
                try {
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    
                    // Decode all segments to AudioBuffers
                    const audioBuffers = [];
                    for (let i = 0; i < audioSegments.length; i++) {
                        const arrayBuffer = await audioSegments[i].arrayBuffer();
                        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                        audioBuffers.push(audioBuffer);
                        console.log(`‚úÖ Decoded segment ${i + 1}/${audioSegments.length}, duration: ${audioBuffer.duration.toFixed(2)}s`);
                    }

                    // Combine all AudioBuffers into one continuous buffer
                    const sampleRate = audioBuffers[0].sampleRate;
                    const numberOfChannels = audioBuffers[0].numberOfChannels;
                    let totalLength = 0;
                    
                    // Calculate total length
                    audioBuffers.forEach(buffer => {
                        totalLength += buffer.length;
                    });

                    // Create combined buffer
                    const combinedBuffer = audioContext.createBuffer(numberOfChannels, totalLength, sampleRate);
                    
                    // Copy each buffer into the combined buffer
                    let offset = 0;
                    audioBuffers.forEach((buffer, index) => {
                        for (let channel = 0; channel < numberOfChannels; channel++) {
                            const channelData = combinedBuffer.getChannelData(channel);
                            const sourceData = buffer.getChannelData(channel);
                            channelData.set(sourceData, offset);
                        }
                        offset += buffer.length;
                        console.log(`  Copied segment ${index + 1}/${audioBuffers.length} (${buffer.length} samples) at offset ${offset - buffer.length}`);
                    });

                    console.log(`‚úÖ Combined ${audioSegments.length} segments into one buffer, total duration: ${combinedBuffer.duration.toFixed(2)}s`);

                    // Convert combined AudioBuffer to WAV
                    audioBlob = audioBufferToWav(combinedBuffer);
                    console.log(`‚úÖ Converted to WAV, size: ${audioBlob.size} bytes`);

                    // For debugging: play the combined audio
                    const audioUrl = URL.createObjectURL(audioBlob);
                    document.getElementById('audioPlayback').src = audioUrl;
                    document.getElementById('audioPlayer').style.display = 'block';
                } catch (error) {
                    console.error('Error combining/converting audio:', error);
                    // Fallback: try simple blob combination
                    audioBlob = new Blob(audioSegments, { type: 'audio/webm' });
                    const audioUrl = URL.createObjectURL(audioBlob);
                    document.getElementById('audioPlayback').src = audioUrl;
                    document.getElementById('audioPlayer').style.display = 'block';
                }
            } else {
                console.warn('‚ö†Ô∏è No audio segments recorded');
                audioBlob = null;
            }

            if (userAudioTrack) {
                userAudioTrack.stop();
                userAudioTrack = null;
            }

            // Calculate and save session metrics
            if (sessionStartTime) {
                const sessionDuration = (sessionEndTime - sessionStartTime) / 1000; // in seconds
                const speechActivity = sessionDuration > 0 ? totalSpeakingDuration / sessionDuration : 0;
                
                // Calculate average speech rate
                const validRates = userMessages
                    .map(m => m.speechRate)
                    .filter(r => r && r > 0);
                const avgSpeechRate = validRates.length > 0
                    ? validRates.reduce((a, b) => a + b, 0) / validRates.length
                    : 0;
                
                // Create record
                const now = new Date();
                const record = {
                    dateTime: now.toISOString(),
                    date: now.toISOString().split('T')[0],
                    time: now.toTimeString().split(' ')[0],
                    duration: Math.round(sessionDuration),
                    speechActivity: parseFloat(speechActivity.toFixed(3)),
                    avgSpeechRate: parseFloat(avgSpeechRate.toFixed(2)),
                    totalWords: totalWordsSpoken,
                    messageCount: userMessages.length,
                    completed: true
                };
                
                // Save to server
                try {
                    const response = await fetch('/api/conversations', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify(record)
                    });
                    if (response.ok) {
                        console.log('‚úÖ Session data saved:', record);
                        updateStatus('idle', 'Session saved!');
                    } else {
                        console.error('Failed to save session data');
                    }
                } catch (error) {
                    console.error('Error saving session data:', error);
                }
            }

            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
            document.getElementById('languageToggle').disabled = false;
            isSessionActive = false;
            serverAudioStream = null;
            messageText = '';
            userSpeakingStartTime = null;
            aiResponseStartTime = null;
            currentUserTranscript = '';
            updateStatus('idle', 'Stopped');

            // Run voice analysis if we have audio
            if (audioBlob) {
                await analyzeAudio();
            }
        }

        // Add message to UI
        function addMessage(role, text, wordCount = null, speakingDuration = null, wordsPerSecond = null) {
            const messagesDiv = document.getElementById('messages');
            
            if (messagesDiv.children.length === 1 && messagesDiv.children[0].textContent.includes('Conversation')) {
                messagesDiv.innerHTML = '';
            }

            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            
            const labelDiv = document.createElement('div');
            labelDiv.className = 'message-label';
            labelDiv.textContent = role === 'user' ? 'üë§ You' : role === 'assistant' ? 'ü§ñ Assistant' : '‚ÑπÔ∏è System';
            
            const textDiv = document.createElement('div');
            textDiv.className = 'message-text';
            textDiv.textContent = text;
            
            messageDiv.appendChild(labelDiv);
            messageDiv.appendChild(textDiv);
            
            // Add metrics for user messages
            if (role === 'user' && (wordCount !== null || speakingDuration !== null || wordsPerSecond !== null)) {
                const metricsDiv = document.createElement('div');
                metricsDiv.className = 'message-metrics';
                const metrics = [];
                if (wordCount !== null) {
                    metrics.push(`${wordCount} word${wordCount !== 1 ? 's' : ''}`);
                }
                if (speakingDuration !== null && speakingDuration !== 'N/A') {
                    metrics.push(`Spoke for ${speakingDuration}`);
                }
                if (wordsPerSecond !== null) {
                    metrics.push(`${wordsPerSecond} words/sec`);
                }
                metricsDiv.textContent = metrics.join(' ‚Ä¢ ');
                messageDiv.appendChild(metricsDiv);
            }
            
            messagesDiv.appendChild(messageDiv);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }

        // Clear messages
        function clearMessages() {
            document.getElementById('messages').innerHTML = '';
        }

        // Update status
        function updateStatus(type, message) {
            const statusDiv = document.getElementById('status');
            statusDiv.className = `status ${type}`;
            statusDiv.textContent = message;
            
            if (type === 'listening') {
                statusDiv.classList.add('pulse');
            } else {
                statusDiv.classList.remove('pulse');
            }
        }

        // Set silence duration preset
        function setSilenceDuration(ms) {
            document.getElementById('silenceDuration').value = ms;
        }

        // Toggle language between English and Chinese
        function toggleLanguage() {
            currentLanguage = currentLanguage === 'en' ? 'zh' : 'en';
            const languageLabel = document.getElementById('languageLabel');
            const languageFlag = document.getElementById('languageFlag');
            
            if (currentLanguage === 'zh') {
                languageLabel.textContent = '‰∏≠Êñá';
                languageFlag.textContent = 'üá®üá≥';
            } else {
                languageLabel.textContent = 'English';
                languageFlag.textContent = 'üá∫üá∏';
            }
        }

        // Normalize prediction labels (CTRL -> Healthy, etc.)
        function normalizePrediction(pred) {
            const predStr = String(pred).toUpperCase().trim();
            if (predStr === 'CTRL' || predStr === 'CONTROL' || predStr === 'HEALTHY') {
                return 'Healthy';
            } else if (predStr === 'MCI' || predStr.includes('MCI')) {
                return 'MCI';
            }
            return pred; // Return original if not recognized
        }

        // Convert AudioBuffer to WAV Blob
        function audioBufferToWav(buffer) {
            const numChannels = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const format = 1; // PCM
            const bitDepth = 16;
            
            const bytesPerSample = bitDepth / 8;
            const blockAlign = numChannels * bytesPerSample;
            
            const length = buffer.length * numChannels * bytesPerSample + 44;
            const arrayBuffer = new ArrayBuffer(length);
            const view = new DataView(arrayBuffer);
            const samples = [];
            
            // Convert float samples to 16-bit PCM
            for (let channel = 0; channel < numChannels; channel++) {
                const channelData = buffer.getChannelData(channel);
                for (let i = 0; i < channelData.length; i++) {
                    const sample = Math.max(-1, Math.min(1, channelData[i]));
                    samples.push(sample < 0 ? sample * 0x8000 : sample * 0x7FFF);
                }
            }
            
            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };
            
            writeString(0, 'RIFF');
            view.setUint32(4, length - 8, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, format, true);
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * blockAlign, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitDepth, true);
            writeString(36, 'data');
            view.setUint32(40, length - 44, true);
            
            // Write samples
            let offset = 44;
            for (let i = 0; i < samples.length; i++) {
                view.setInt16(offset, samples[i], true);
                offset += 2;
            }
            
            return new Blob([arrayBuffer], { type: 'audio/wav' });
        }

        async function analyzeAudio() {
            try {
                // Step 1: Analyze voice features (if audio available)
                if (!audioBlob) {
                    console.warn('No audio blob available for voice analysis');
                    return;
                }

                updateStatus('processing', 'üîÑ Analyzing voice features...');

                // Convert blob to File for FormData
                const audioFile = new File([audioBlob], 'recording.wav', { type: 'audio/wav' });
                const formData = new FormData();
                formData.append('audio', audioFile);

                const response = await fetch(API_ENDPOINT, {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`Server error: ${response.status} - ${errorText}`);
                }

                const result = await response.json();
                
                // Display results
                const prediction = result.prediction || result.pred || 'Unknown';
                const normalizedPrediction = normalizePrediction(prediction);
                
                const predictedClassDiv = document.getElementById('predictedClass');
                predictedClassDiv.innerHTML = '';
                
                // Add icon
                const icon = document.createElement('span');
                icon.className = `result-icon ${normalizedPrediction.toLowerCase()}`;
                icon.textContent = normalizedPrediction === 'Healthy' ? '‚úÖ' : '‚ö†Ô∏è';
                predictedClassDiv.appendChild(icon);
                
                // Add text
                const text = document.createElement('span');
                text.textContent = normalizedPrediction;
                predictedClassDiv.appendChild(text);
                
                // Show description modal
                const descriptionModal = document.getElementById('descriptionModal');
                const descriptionTitle = document.getElementById('descriptionTitle');
                const descriptionText = document.getElementById('descriptionText');
                
                if (normalizedPrediction === 'Healthy') {
                    descriptionTitle.textContent = '‚úÖ Healthy Cognitive Status';
                    descriptionText.textContent = 'Your voice analysis indicates healthy cognitive function. This is a positive result, but please remember this is a screening tool and not a medical diagnosis.';
                    descriptionModal.style.display = 'block';
                } else if (normalizedPrediction === 'MCI') {
                    descriptionTitle.textContent = '‚ö†Ô∏è Mild Cognitive Impairment (MCI)';
                    descriptionText.textContent = 'Your voice analysis suggests possible mild cognitive impairment. This is a screening result and should be discussed with a healthcare professional for proper evaluation.';
                    descriptionModal.style.display = 'block';
                } else {
                    descriptionModal.style.display = 'none';
                }
                
                // Display probabilities as horizontal bar chart
                if (result.probabilities) {
                    const probSection = document.getElementById('probabilitiesSection');
                    const probContainer = document.getElementById('probabilities');
                    probContainer.innerHTML = '';
                    
                    // Sort probabilities by value (highest first)
                    const sortedProbs = Object.entries(result.probabilities)
                        .sort((a, b) => b[1] - a[1]);
                    
                    sortedProbs.forEach(([label, value]) => {
                        const normalizedLabel = normalizePrediction(label);
                        const percentage = (value * 100).toFixed(1);
                        const barClass = normalizedLabel.toLowerCase();
                        
                        const probContainerItem = document.createElement('div');
                        probContainerItem.className = 'probability-bar-container';
                        probContainerItem.innerHTML = `
                            <div class="probability-bar-label">
                                <span>${normalizedLabel}</span>
                                <span>${percentage}%</span>
                            </div>
                            <div class="probability-bar-wrapper">
                                <div class="probability-bar ${barClass}" style="width: 0%;" data-width="${percentage}">
                                    ${percentage}%
                                </div>
                            </div>
                        `;
                        probContainer.appendChild(probContainerItem);
                    });
                    
                    // Animate bars after a short delay
                    setTimeout(() => {
                        const bars = probContainer.querySelectorAll('.probability-bar');
                        bars.forEach(bar => {
                            const width = bar.getAttribute('data-width');
                            bar.style.width = width + '%';
                        });
                    }, 100);
                    
                    probSection.style.display = 'block';
                } else {
                    document.getElementById('probabilitiesSection').style.display = 'none';
                }

                document.getElementById('voiceAnalysisResults').classList.add('active');
                updateStatus('complete', '‚úÖ Voice analysis complete! View your results below.');
                console.log('‚úÖ Voice analysis complete');
            } catch (error) {
                console.error('Error analyzing audio:', error);
                updateStatus('idle', '‚ùå Error analyzing audio: ' + error.message);
                alert('Error analyzing audio: ' + error.message);
            }
        }

        // Initialize on page load
        window.addEventListener('load', async () => {
            // API key is loaded from server when needed
        });
    </script>
</body>
</html>


